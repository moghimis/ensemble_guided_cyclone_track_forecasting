{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate test data for iterative ensemble algorithms\n",
    "\n",
    "Test data is of the GEFS forecasts of the 2018 hurricanes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta  \n",
    "import os\n",
    "os.environ[\"PROJ_LIB\"] = \"C:\\\\ProgramData\\\\Anaconda3\\\\Library\\\\share\";\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lat_lon(lat, lon):\n",
    "    '''\n",
    "        parse the latitude, longitude values \n",
    "        input is an array of the format 214N, 859W \n",
    "        (lat: 0 - 900 tenths of degrees, lon: 0 - 900 tenths of degrees,)\n",
    "    '''   \n",
    "    lat_ = []\n",
    "    lon_ = []\n",
    "    for x in lat:\n",
    "        if 'N' in x:\n",
    "            lat_.append(float(x.strip('N')))\n",
    "        elif 'S' in x:\n",
    "            lat_.append(-1*float(x.strip('S')))\n",
    "    \n",
    "    for x in lon:\n",
    "        if 'E' in x:\n",
    "            lon_.append(float(x.strip('E')))\n",
    "        elif 'W' in x:\n",
    "            lon_.append(-1*float(x.strip('W')))\n",
    "\n",
    "    lat_ = np.array(lat_)/10.0\n",
    "    lon_ = np.array(lon_)/10.0\n",
    "    \n",
    "    return lat_, lon_\n",
    "\n",
    "def normalized_xy_Coords(lat, lon, m):\n",
    "    nF = max(m.xmax, m.ymax)\n",
    "\n",
    "    x, y = np.array(m(lon,lat,inverse=False))\n",
    "    \n",
    "    return [x/nF, y/nF]\n",
    "\n",
    "def xy_Coords(lat, lon, m):\n",
    "\n",
    "    x, y = np.array(m(lon,lat,inverse=False))\n",
    "    \n",
    "    return [x, y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: MatplotlibDeprecationWarning: \n",
      "The dedent function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use inspect.cleandoc instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9173846.41491117\n"
     ]
    }
   ],
   "source": [
    "# Set some parameters\n",
    "wdir = 'C:/Users/vravindr/Dropbox/my_workspace/tracking storms/'\n",
    "\n",
    "data_dir = wdir + 'data/2018/'\n",
    "ens_data_dir = data_dir + 'ensemble/'\n",
    "bt_data_dir = data_dir + 'best_track/'\n",
    "\n",
    "xfn = ['al012018', 'al022018', 'al032018', 'al042018', 'al052018', 'al062018', 'al072018', 'al082018',\n",
    "       'al092018', 'al102018', 'al112018', 'al122018', 'al132018', 'al142018', 'al152018', 'al162018']\n",
    "basin ='Al'\n",
    "Storms = ['ALBERTO', 'BERYL', 'CHRIS', 'DEBBY', 'ERNESTO', 'FLORENCE', 'GORDON', 'HELENE',\n",
    "          'ISAAC', 'JOYCE', 'ELEVEN', 'KIRK', 'LESLIE', 'MICHAEL', 'NADINE', 'OSCAR']\n",
    "\n",
    "\n",
    "'''\n",
    "al012018 ALBERTO\n",
    "al022018 BERYL\n",
    "al032018 CHRIS\n",
    "al042018 DEBBY \n",
    "al052018 ERNESTO\n",
    "al062018 FLORENCE\n",
    "al072018 GORDON\n",
    "al082018 HELENE\n",
    "al092018 ISAAC\n",
    "al102018 JOYCE\n",
    "al112018 ELEVEN\n",
    "al122018 KIRK\n",
    "al132018 LESLIE\n",
    "al142018 MICHAEL\n",
    "al152018 NADINE\n",
    "al162018 OSCAR\n",
    "'''    \n",
    "# Storm to consider (all the forecasts of the storm are used)\n",
    "ens_tn = ['AC00', 'AP01','AP02','AP03','AP04','AP05','AP06','AP07','AP08','AP09','AP10','AP11','AP12','AP13','AP14',\n",
    "          'AP15','AP16','AP17','AP18','AP19','AP20']\n",
    "ens_mean_tn = ['AEMN'] # ensemble mean track name\n",
    "best_tn = ['BEST']\n",
    "\n",
    "forecast_periods = [0, 6, 12, 18, 24, 30, 36, 42, 48] # forecast horizon is 48hrs, and time step is 6 hrs, origin is from 0hrs\n",
    "\n",
    "nens = len(ens_tn)\n",
    "ntst = len(forecast_periods) # number of time steps. 6hrs is the time step for the GEFS forecasts.\n",
    "\n",
    "    \n",
    "if(basin == 'Al'):\n",
    "    # North Atlantic basin borders: lat: 0 deg to 50 deg, lon: 10W to 100W\n",
    "    # Choose standard parallels: 16.67N and  33.33N, central lon: 45W, llcrnrlon=-100 , llcrnrlat=0, urcrnrlon=-10, urcrnrlat=50\n",
    "    proj = Basemap(resolution='l',projection='eqdc',\\\n",
    "                lat_1=16.67,lat_2=33.33,lon_0=-45.0,\n",
    "                llcrnrlon=-100 , llcrnrlat=0 , urcrnrlon=-10, urcrnrlat=50  )\n",
    "\n",
    "nF = max(proj.xmax, proj.ymax) # normalizing factor of the coordinates (X, Y) used later\n",
    "print(nF)\n",
    "\n",
    "cols_names = ['BASIN', 'CY', 'YYYYMMDDHH', 'TECHNUM/MIN', 'TECH', 'TAU', 'LatN/S', 'LonE/W', 'VMAX', 'MSLP', 'TY', 'RAD', 'WINDCODE', \n",
    "        'RAD1', 'RAD2', 'RAD3', 'RAD4', 'POUTER', 'ROUTER', 'RMW', 'GUSTS', 'EYE', 'SUBREGION', 'MAXSEAS', 'INITIALS', 'DIR', \n",
    "        'SPEED', 'STORMNAME', 'DEPTH', 'SEAS', 'SEASCODE', 'SEAS1', 'SEAS2', 'SEAS3', 'SEAS4', 'USERDEFINED1', 'userdata1',\n",
    "        'USERDEFINED2', 'userdata2', 'USERDEFINED3', 'userdata3', 'USERDEFINED4', 'userdata4', 'USERDEFINED5', 'userdata5']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "    ALBERTO\n",
      "2\n",
      "    ALBERTO\n",
      "3\n",
      "    ALBERTO\n",
      "4\n",
      "    ALBERTO\n",
      "5\n",
      "    ALBERTO\n",
      "6\n",
      "    ALBERTO\n",
      "7\n",
      "    ALBERTO\n",
      "8\n",
      "    ALBERTO\n",
      "9\n",
      "    ALBERTO\n",
      "10\n",
      "    ALBERTO\n",
      "11\n",
      "    ALBERTO\n",
      "12\n",
      "    ALBERTO\n",
      "13\n",
      "    ALBERTO\n",
      "14\n",
      "    ALBERTO\n",
      "15\n",
      "    ALBERTO\n",
      "16\n",
      "      BERYL\n",
      "17\n",
      "      BERYL\n",
      "18\n",
      "      BERYL\n",
      "19\n",
      "      BERYL\n",
      "20\n",
      "      BERYL\n",
      "21\n",
      "      BERYL\n",
      "22\n",
      "      BERYL\n",
      "23\n",
      "      BERYL\n",
      "24\n",
      "      BERYL\n",
      "25\n",
      "      BERYL\n",
      "26\n",
      "      BERYL\n",
      "27\n",
      "      BERYL\n",
      "28\n",
      "      BERYL\n",
      "29\n",
      "      BERYL\n",
      "30\n",
      "      BERYL\n",
      "31\n",
      "      BERYL\n",
      "32\n",
      "      BERYL\n",
      "33\n",
      "      BERYL\n",
      "34\n",
      "      BERYL\n",
      "35\n",
      "      BERYL\n",
      "36\n",
      "      BERYL\n",
      "37\n",
      "      BERYL\n",
      "38\n",
      "      BERYL\n",
      "39\n",
      "      BERYL\n",
      "40\n",
      "      BERYL\n",
      "41\n",
      "      BERYL\n",
      "42\n",
      "      BERYL\n",
      "43\n",
      "      BERYL\n",
      "44\n",
      "      BERYL\n",
      "45\n",
      "      BERYL\n",
      "46\n",
      "      BERYL\n",
      "47\n",
      "      BERYL\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-37bda5728fd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# populate the Storm name in rows which dont have it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mstorm_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mens_data2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'STORMNAME'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# initialize storm name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mens_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'STORMNAME'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mens_data2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m27\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mstorm_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36miterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    844\u001b[0m         \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                 data = sanitize_array(data, index, dtype, copy,\n\u001b[1;32m--> 262\u001b[1;33m                                       raise_cast_failure=True)\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'object'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m         \u001b[0minferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minferred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'period'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.infer_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib._try_infer_map\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[1;34m(dtype)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;31m# append bit counts to str, unicode, and void\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflexible\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_isunsized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[1;34m(arg1, arg2)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \"\"\"\n\u001b[1;32m--> 392\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m         \u001b[0marg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datafn = 0\n",
    "\n",
    "ensemble_data = np.ndarray(shape=(1, nens*ntst), dtype=float)\n",
    "mean_ens_data = np.ndarray(shape=(1, ntst), dtype=float)\n",
    "bt_label_data = np.ndarray(shape=(1, ntst), dtype=float)\n",
    "\n",
    "nD = 0 # number of data-sets collected\n",
    "    \n",
    "for _xfn in xfn: # loop over all the avialble data files\n",
    "    \n",
    "    ens_fn = ens_data_dir + 'a' + _xfn + '.dat'\n",
    "    bt_fn = bt_data_dir + 'b' + _xfn + '.dat'\n",
    "\n",
    "    ens_data = pd.read_csv(ens_fn,names=cols_names, engine='python')\n",
    "    bt_data = pd.read_csv(bt_fn,names=cols_names, engine='python')\n",
    "\n",
    "    # parse and replace the latitude and longitude values\n",
    "    [lat, lon] = parse_lat_lon(ens_data['LatN/S'], ens_data['LonE/W'])\n",
    "    ens_data['LatN/S'] = lat\n",
    "    ens_data['LonE/W'] = lon\n",
    "\n",
    "    [lat, lon] = parse_lat_lon(bt_data['LatN/S'], bt_data['LonE/W'])\n",
    "    bt_data['LatN/S'] = lat\n",
    "    bt_data['LonE/W'] = lon\n",
    "\n",
    "    ###################################################################################\n",
    "\n",
    "    ens_data2 = copy.deepcopy(ens_data)\n",
    "\n",
    "    # populate the Storm name in rows which dont have it\n",
    "    storm_name = ens_data2.iloc[0]['STORMNAME'] # initialize storm name\n",
    "    for index, row in ens_data.iterrows():\n",
    "        if(row['STORMNAME'] is None):\n",
    "            ens_data2.iat[index, 27]= storm_name\n",
    "        else:\n",
    "            storm_name = row['STORMNAME'] \n",
    "\n",
    "    ens_data2 = ens_data2.loc[ens_data2['TAU']>= min(forecast_periods)]\n",
    "    ens_data2 = ens_data2.loc[ens_data2['TAU']<= max(forecast_periods)]\n",
    "\n",
    "    # Add X, Y coordinates to the dataframes\n",
    "    [X, Y] = xy_Coords(np.array(ens_data2['LatN/S']),np.array(ens_data2['LonE/W']), proj)\n",
    "    ens_data2.insert(len(ens_data2.columns), \"X\", X) \n",
    "    ens_data2.insert(len(ens_data2.columns), \"Y\", Y) \n",
    "\n",
    "    [X, Y] = xy_Coords(np.array(bt_data['LatN/S']),np.array(bt_data['LonE/W']), proj)\n",
    "    bt_data.insert(len(bt_data.columns), \"X\", X) \n",
    "    bt_data.insert(len(bt_data.columns), \"Y\", Y) \n",
    "    ########################################################################################\n",
    "\n",
    "  \n",
    "    st = Storms[datafn]\n",
    "    # extract the data containing the storm name\n",
    "    ens_data3 = ens_data2[ens_data2['STORMNAME'].str.contains(st)]\n",
    "\n",
    "\n",
    "    # find unique forecasts\n",
    "    forecasts = ens_data3['YYYYMMDDHH'].unique()\n",
    "\n",
    "    # regrid the (lat, lon) of the storms to (X,Y), and add the (X,Y) cols\n",
    "\n",
    "    d1 = copy.deepcopy(ens_data3)    \n",
    "\n",
    "    ''' loop over each forecast of the data file'''\n",
    "    for fc in forecasts:\n",
    "\n",
    "        d2 = d1[d1['YYYYMMDDHH'] == fc]\n",
    "\n",
    "        # extract set of [ensembles, mean ensemble, best-track] of this forecast, place them one below another\n",
    "        ens_d = pd.DataFrame(columns=['YYYYMMDDHH', 'TECH', 'LatN/S', 'LonE/W'])\n",
    "        bt_d = pd.DataFrame(columns=['YYYYMMDDHH', 'TECH', 'LatN/S', 'LonE/W'])   \n",
    "        tracks = ens_tn + ens_mean_tn + best_tn\n",
    "        for tn in tracks:\n",
    "            d3 = copy.deepcopy(d2)\n",
    "            d3 = d3[(d3['TECH'].str.contains(tn))]  \n",
    "            d3 = d3.drop_duplicates(subset =\"TAU\") # drop rows with duplicate forecast period\n",
    "            d3 = d3.sort_values(by='TAU', ascending=True) # make sure they are in ascending order\n",
    "            ens_d = ens_d.append(d3, sort=False)\n",
    "            # for best tracks there is no \"forecast\" date, rather the date in the column corresponds to the date of the track info\n",
    "            if(tn =='BEST'):\n",
    "                bd1 = copy.deepcopy(bt_data)\n",
    "                fc_dt = datetime.strptime(str(fc), '%Y%m%d%H')\n",
    "                array = []\n",
    "                for fp in forecast_periods:\n",
    "                    dt = fc_dt + timedelta(hours = fp)\n",
    "                    array.append(dt.strftime('%Y%m%d%H'))\n",
    "                bd1 = bd1.loc[bd1['YYYYMMDDHH'].isin(array)]\n",
    "                bd1 = bd1.drop_duplicates(subset =\"YYYYMMDDHH\") # drop rows with duplicate dates\n",
    "                bd1 = bd1.sort_values(by='YYYYMMDDHH', ascending=True) # make sure they are in ascending order\n",
    "\n",
    "        if(len(ens_d) == (nens+1)*ntst and len(bd1)==ntst): # sometimes some enseble tracks may not have certain forecasts, \n",
    "            # and/or best track data may not be avialable. In that case the data is not added to the dataset.\n",
    "            nD = nD + 1\n",
    "            ed = ens_d[~ens_d['TECH'].str.contains(ens_mean_tn[0])]\n",
    "            med = ens_d[ens_d['TECH'].str.contains(ens_mean_tn[0])]\n",
    "            if(nD ==1): # very first dataset\n",
    "                ensemble_data = ed.filter(['YYYYMMDDHH', 'TECH', 'LatN/S', 'LonE/W', 'X', 'Y']).to_numpy()\n",
    "                mean_ens_data = med.filter(['YYYYMMDDHH', 'TECH', 'LatN/S', 'LonE/W', 'X', 'Y']).to_numpy()\n",
    "                bt_label_data = bd1.filter(['YYYYMMDDHH', 'TECH', 'LatN/S', 'LonE/W', 'X', 'Y']).to_numpy()\n",
    "                print(nD)\n",
    "                print(bd1['STORMNAME'].iloc[0])\n",
    "            else:\n",
    "                ensemble_data = np.vstack((ensemble_data, ed.filter(['YYYYMMDDHH', 'TECH', 'LatN/S', 'LonE/W', 'X', 'Y']).to_numpy()))\n",
    "                mean_ens_data = np.vstack((mean_ens_data, med.filter(['YYYYMMDDHH', 'TECH', 'LatN/S', 'LonE/W', 'X', 'Y']).to_numpy()))\n",
    "                bt_label_data = np.vstack((bt_label_data, bd1.filter(['YYYYMMDDHH', 'TECH', 'LatN/S', 'LonE/W', 'X', 'Y']).to_numpy()))\n",
    "                print(nD)\n",
    "                print(bd1['STORMNAME'].iloc[0])\n",
    "        \n",
    "    ''' End loop over forecasts '''\n",
    "    datafn = datafn + 1 # increment data file number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data sets collected:  318\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data sets collected: \", nD)\n",
    "\n",
    "if(nD>0):\n",
    "    ensemble_data = ensemble_data.reshape(nD,nens*ntst,6)\n",
    "    mean_ens_data = mean_ens_data.reshape(nD,ntst,6)\n",
    "    bt_label_data = bt_label_data.reshape(nD,ntst,6)\n",
    "\n",
    "    with open('2018_ensTest_6hrs.pickle', 'wb') as handle:\n",
    "        pickle.dump(ensemble_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('2018_meanEnsTest_48hrs.pickle', 'wb') as handle:\n",
    "        pickle.dump(mean_ens_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('2018_bestLabelTest_48hrs.pickle', 'wb') as handle:\n",
    "        pickle.dump(bt_label_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('ensTest.pickle', 'rb') as handle:\\n    ensemble_data = pickle.load(handle)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with open('ensTest.pickle', 'rb') as handle:\n",
    "    ensemble_data = pickle.load(handle)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
